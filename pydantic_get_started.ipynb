{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdLVGi+A9kDSK2fvr4ZDFK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarforever/pydantic-tutorials/blob/main/pydantic_get_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Started with PyDantic AI\n",
        "\n",
        "Ever used FastAPI, LangChain, or the OpenAI Python SDK? Then you've already used [Pydantic](https://pydantic.dev/) under the hood. Pydantic is Python's go-to library for data validation, used by thousands of packages to ensure data matches expected types and formats.\n",
        "What makes Pydantic special is its use of standard Python type hints. Instead of learning a new syntax, you just write regular Python code with type annotations, and Pydantic handles the validation:\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class User(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    email: str\n",
        "\n",
        "json_string = '{\"name\": \"John Doe\", \"age\": 30, \"email\": \"john@example.com\"}'\n",
        "user = User.parse_raw(json_string)\n",
        "\n",
        "```\n",
        "\n",
        "[Pydantic AI](https://ai.pydantic.dev/) builds on this foundation to make building AI applications just as straightforward. Created by the Pydantic team, it provides a framework for working with large language models (LLMs) that feels natural to Python developers.\n",
        "\n",
        "Key features of Pydantic AI:\n",
        "- Works with major LLM providers (OpenAI, Anthropic, Gemini, Groq)\n",
        "- Uses standard Python for control flow and composition\n",
        "- Validates AI responses using Pydantic models\n",
        "- Supports streaming responses with validation\n",
        "- Includes built-in debugging and monitoring\n",
        "\n",
        "In this tutorial, we'll explore how to use Pydantic AI to build reliable AI applications using familiar Python patterns. Let's get started!"
      ],
      "metadata": {
        "id": "DbzujyM2Pc9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "The only Python package you need for now is `pydantic_ai`."
      ],
      "metadata": {
        "id": "fjI3ajM8fc5q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxQbhxKTypJg",
        "outputId": "93a11690-898e-40c9-b7c4-628c44d0f8ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.37.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pydantic_ai -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Colab Environment Ready\n",
        "\n",
        "To make the demo application run, we will also need `nest-asyncio`.\n",
        "\n",
        "Next step is to set up environmental variable `OPENAI_API_KEY` so that the Pydantic AI Agents can pick it up in using OpenAI models."
      ],
      "metadata": {
        "id": "spdjK3wrfmFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest-asyncio -qU"
      ],
      "metadata": {
        "id": "4eNx6IKlzZgB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "BZtoNwxczdux"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "lQUPHTYgzGiM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pydantic AI Agents\n",
        "\n",
        "Let's start looking into some cool examples of Pydantic AI agents."
      ],
      "metadata": {
        "id": "XRmerIQOgBE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Simplest One\n",
        "\n",
        "Chat with OpenAI `gpt-4o` straight away."
      ],
      "metadata": {
        "id": "z-gg8vLMgKwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent"
      ],
      "metadata": {
        "id": "Ywyz_i_9y4SC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\"openai:gpt-4o\")\n",
        "response = agent.run_sync(\"Hey, dude!\")\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muzkRuKj867U",
        "outputId": "48780d98-44b8-452d-f882-e216a1a8b9f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey there! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent with Static Prompt"
      ],
      "metadata": {
        "id": "5xfsoAoAgdOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\"openai:gpt-4o\", system_prompt=\"You can only speak Chinese\")\n",
        "response = agent.run_sync(\"Hey, dude!\")\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNuOO7a69Z_U",
        "outputId": "d9fb84ad-96df-44d0-ee35-a878352142f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好！有什么我可以帮助你的吗？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent with Dynamic Prompt"
      ],
      "metadata": {
        "id": "IfMRzGDngpPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent, RunContext"
      ],
      "metadata": {
        "id": "DXr9G0SC96Em"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_prompt_agent = Agent(\"openai:gpt-4o\")\n",
        "\n",
        "@dynamic_prompt_agent.system_prompt\n",
        "def set_agent_name(ctx: RunContext[str]) -> str:\n",
        "    return f\"Your name is {ctx.deps}.\"\n",
        "\n",
        "response = dynamic_prompt_agent.run_sync(\"Hey, dude! Who are you?\", deps=\"Jarvis\")\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2XHmZke9spd",
        "outputId": "5bc52c96-1ef5-4ea4-c298-72f221338b09"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey there! I'm Jarvis, your AI assistant. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent with Dependency Type"
      ],
      "metadata": {
        "id": "Q2XEz8rlhAs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Player:\n",
        "    name: str\n",
        "    goals: int\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "    'openai:gpt-4o',\n",
        "    deps_type=Player,\n",
        "    result_type=bool,\n",
        ")\n",
        "\n",
        "@agent.system_prompt\n",
        "def add_player_name(ctx: RunContext[Player]) -> str:\n",
        "    player_name = ctx.deps.name\n",
        "    return f\"The player's name is {player_name}.\"\n",
        "\n",
        "@agent.system_prompt\n",
        "def add_player_goals(ctx: RunContext[Player]) -> str:\n",
        "    goals = ctx.deps.goals\n",
        "    return f\"The player's goals so far is {goals}.\"\n",
        "\n",
        "response = agent.run_sync(\"Hey, dude! Does the player ever score a goal?\", deps=Player(name=\"Messi\", goals=2))\n",
        "print(response.data)\n",
        "\n",
        "response = agent.run_sync(\"Hey, dude! Does the player ever score a goal?\", deps=Player(name=\"Ronaldo\", goals=0))\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M9B-U5IKGV6",
        "outputId": "0dd8014a-44b9-42c0-8614-116c8c1450c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent with Function Tools\n",
        "\n",
        "Function tools provide a mechanism for models to retrieve extra information to help them generate a response.\n",
        "\n",
        "Developers use decorators `@agent.tool_plain` or `@agent.tool` to define tools."
      ],
      "metadata": {
        "id": "ITkIu7KKhLdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent('openai:gpt-4o')\n",
        "\n",
        "@agent.tool\n",
        "def get_player_goals(ctx: RunContext[str], player_name: str) -> str:\n",
        "    print(f\"Getting the goals of player {player_name} so far\")\n",
        "    if player_name == 'Messi':\n",
        "        return '2'\n",
        "    elif player_name == 'Ronaldo':\n",
        "        return '100'\n",
        "    else:\n",
        "        return '0'\n",
        "\n",
        "response = agent.run_sync(\"Let me know if Ronaldo scored so far\")\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MvdJo3TNZWm",
        "outputId": "7d0e8ac0-c2c3-42db-d5af-48ffc23f9a75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting the goals of player Ronaldo so far\n",
            "Ronaldo has scored 100 goals so far.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.all_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvRxFW8r7LsG",
        "outputId": "4e5ecef7-06b9-4c90-9c79-46eaa4c03477"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserPrompt(content='Let me know if Ronaldo scored so far', timestamp=datetime.datetime(2024, 12, 13, 21, 28, 29, 504173, tzinfo=datetime.timezone.utc), role='user'),\n",
              " ModelStructuredResponse(calls=[ToolCall(tool_name='get_player_goals', args=ArgsJson(args_json='{\"player_name\":\"Ronaldo\"}'), tool_id='call_31XYZmv6dW8mOsEGatfpP7Zi')], timestamp=datetime.datetime(2024, 12, 13, 21, 28, 29, tzinfo=datetime.timezone.utc), role='model-structured-response'),\n",
              " ToolReturn(tool_name='get_player_goals', content='100', tool_id='call_31XYZmv6dW8mOsEGatfpP7Zi', timestamp=datetime.datetime(2024, 12, 13, 21, 28, 30, 253595, tzinfo=datetime.timezone.utc), role='tool-return'),\n",
              " ModelTextResponse(content='Ronaldo has scored 100 goals so far.', timestamp=datetime.datetime(2024, 12, 13, 21, 28, 30, tzinfo=datetime.timezone.utc), role='model-text-response')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run_sync(\"Let me know if Saka scored so far\")\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fGvNFz6PPJv",
        "outputId": "461418a0-38ef-4a95-c3a0-de685b696b78"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting the goals of player Saka so far\n",
            "Saka has not scored any goals so far.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.all_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjURG5FIOqsY",
        "outputId": "0b9c26ef-9e97-4293-eb82-43972e65f1e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserPrompt(content='Let me know if Saka scored so far', timestamp=datetime.datetime(2024, 12, 13, 21, 30, 3, 734955, tzinfo=datetime.timezone.utc), role='user'),\n",
              " ModelStructuredResponse(calls=[ToolCall(tool_name='get_player_goals', args=ArgsJson(args_json='{\"player_name\":\"Saka\"}'), tool_id='call_gsWKZgcj67OIi2D7F1UfgMMx')], timestamp=datetime.datetime(2024, 12, 13, 21, 30, 3, tzinfo=datetime.timezone.utc), role='model-structured-response'),\n",
              " ToolReturn(tool_name='get_player_goals', content='0', tool_id='call_gsWKZgcj67OIi2D7F1UfgMMx', timestamp=datetime.datetime(2024, 12, 13, 21, 30, 4, 370336, tzinfo=datetime.timezone.utc), role='tool-return'),\n",
              " ModelTextResponse(content='Saka has not scored any goals so far.', timestamp=datetime.datetime(2024, 12, 13, 21, 30, 4, tzinfo=datetime.timezone.utc), role='model-text-response')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}